{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8e5895-30a8-4a2b-82d2-d77ada5542f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install praw mlflow kfp-kubernetes==2.14.0 -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d1b3128-ac06-4190-aba3-63fa64c03131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_141/2114720667.py:3: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2.dsl import importer, Metrics\n",
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import os\n",
    "from kfp.v2.dsl import importer, Metrics\n",
    "from typing import NamedTuple, List\n",
    "from kfp.dsl import Input, component, pipeline\n",
    "from kfp.dsl import OutputPath, Artifact\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset, Output, HTML\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "MLFLOW_RUN_NAME = \"reddit\"\n",
    "MLFLOW_MODEL_NAME = \"reddit-transformer\"\n",
    "client_id=os.getenv('REDDIT_CLIENT_ID')\n",
    "client_secret=os.getenv('REDDIT_CLIENT_PW')\n",
    "# Optional: For authenticated requests\n",
    "username=os.getenv('REDDIT_USER')\n",
    "password=os.getenv('REDDIT_PW')\n",
    "storage_uri = os.getenv(\"REDDIT_DB_URI\")\n",
    "mlflow_tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "mlflow_s3_endpoint_url = os.getenv('MLFLOW_S3_ENDPOINT_URL')\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911fc81-333c-4530-bebd-7fa43bfb55df",
   "metadata": {},
   "source": [
    "# Downloading Data\n",
    "----\n",
    "\n",
    "This component downloads data from reddit and saves them to the datastore for down stream tasks. \n",
    "\n",
    "It is ran daily to collect the daily limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b7c7e24-3b0b-4baf-b9bb-64b9c3783e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"praw\", \"pandas\"]\n",
    ")\n",
    "def download_dataset( ) -> None:\n",
    "    import praw\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_PW'),\n",
    "        user_agent=\"YOUR_USER_AGENT\",\n",
    "        # Optional: For authenticated requests\n",
    "        username=os.getenv('REDDIT_USER'),\n",
    "        password=os.getenv('REDDIT_PW')\n",
    "    )\n",
    "    subreddit_names = [\n",
    "        \"funny\",\n",
    "        \"AskReddit\",\n",
    "        \"gaming\",\n",
    "        \"worldnews\",\n",
    "        \"todayilearned\",\n",
    "        \"Music\",\n",
    "        \"aww\",\n",
    "        \"movies\",\n",
    "        \"memes\",\n",
    "        \"science\"\n",
    "    ]\n",
    "    data_path = Path(\"/data\")\n",
    "    submissions = []\n",
    "    \n",
    "    for subreddit_name in subreddit_names:\n",
    "        print(subreddit_name)\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        for submission in subreddit.top(time_filter=\"day\", limit=10):\n",
    "        \n",
    "            most_upvoted_comment = None\n",
    "            highest_score = -1\n",
    "            \n",
    "            for comment in submission.comments.list():\n",
    "                if not hasattr(comment, \"author\") or not comment.author:  # Skip deleted comments\n",
    "                    continue\n",
    "                if comment.score > highest_score:\n",
    "                    highest_score = comment.score\n",
    "                    most_upvoted_comment = comment\n",
    "            text = submission.selftext_html\n",
    "            if most_upvoted_comment is not None:\n",
    "                comment_text = most_upvoted_comment.body.strip()\n",
    "                comment_score = most_upvoted_comment.score\n",
    "            else:\n",
    "                comment_text = \"\"\n",
    "                comment_score = 0\n",
    "            url = submission.url\n",
    "            title = submission.title\n",
    "            id  = submission.id\n",
    "            submission_data = {\n",
    "                \"id\": id,\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"text\": text,\n",
    "                \"top_comment\": comment_text,\n",
    "                \"comment_score\": comment_score,\n",
    "                \"date_added\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            }\n",
    "            submissions.append(submission_data)\n",
    "    new_data = pd.DataFrame(submissions)\n",
    "    data_file = data_path / \"reddit.csv\"\n",
    "    if data_file.exists():\n",
    "        existing_data = pd.read_csv(data_path / \"reddit.csv\")\n",
    "        already_seen_mask = ~new_data['id'].isin(existing_data['id'])\n",
    "        new_data = new_data[already_seen_mask]\n",
    "        \n",
    "        print(f\"New Data: {len(new_data)}\")\n",
    "        print(f\"Existing Data: {len(existing_data)}\")\n",
    "        pd.concat([existing_data, new_data]).to_csv(data_file)\n",
    "    else:\n",
    "        new_data.to_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be6b4b-dba3-4f38-b667-d27dc4b54348",
   "metadata": {},
   "source": [
    "# Test Model\n",
    "----\n",
    "\n",
    "Test if a model's performance has degraded on the last \"test_window\" days of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581e1d7e-d4a1-4c92-a74f-60fabe3bafe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dsl.component\n",
    "def print_comp(text: str):\n",
    "    print(text)\n",
    "    \n",
    "@component(\n",
    "    base_image=\"wallies/python-cuda:3.11-cuda12.2-runtime\",\n",
    "    packages_to_install=[\"ajperry_pipeline>=0.1.15\", \"torch==2.3.0\", \"transformers\",\"mlflow\", \"pyarrow\", \"boto3\", \"torchtext==0.18.0\"]\n",
    ")\n",
    "def test_model(\n",
    "    batch_size: int,\n",
    "    num_epochs: int,\n",
    "    lr: float, \n",
    "    seq_len: int,\n",
    "    d_model: int, \n",
    "    data_folder: str,\n",
    "    model_folder: str, \n",
    "    model_basename: str, \n",
    "    tokenizer_file: str, \n",
    "    experiment_name: str, \n",
    "    num_examples: int, \n",
    "    verbose: bool,\n",
    "    test_window: int\n",
    ") -> bool:\n",
    "    from ajperry_pipeline.ml.utils.reddit import test\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"seq_len\": seq_len,\n",
    "        \"d_model\": d_model,\n",
    "        \"data_folder\": data_folder,\n",
    "        \"model_folder\": model_folder,\n",
    "        \"model_basename\": model_basename,\n",
    "        \"tokenizer_file\": tokenizer_file,\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"num_examples\": num_examples,\n",
    "        \"verbose\": verbose,\n",
    "        \"test_window\": test_window\n",
    "    }\n",
    "    return test(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fb676-7bcd-42fb-95b7-9e575e735504",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "----\n",
    "\n",
    "Train a model using GPU. A customizable number of epochs, learning rate, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba793efb-e7b9-49d2-844c-c05f36912467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "@component(\n",
    "    base_image=\"wallies/python-cuda:3.11-cuda12.2-runtime\",\n",
    "    packages_to_install=[\"ajperry_pipeline>=0.1.15\", \"torch==2.3.0\", \"transformers\",\"mlflow\", \"pyarrow\", \"boto3\", \"torchtext==0.18.0\"]\n",
    ")\n",
    "def train_reddit_model(\n",
    "    batch_size: int,\n",
    "    num_epochs: int,\n",
    "    lr: float, \n",
    "    seq_len: int,\n",
    "    d_model: int, \n",
    "    data_folder: str,\n",
    "    model_folder: str, \n",
    "    model_basename: str, \n",
    "    tokenizer_file: str, \n",
    "    experiment_name: str, \n",
    "    num_examples: int, \n",
    "    verbose: bool,\n",
    "    finetune: str\n",
    "):\n",
    "    from ajperry_pipeline.ml.utils.reddit import train\n",
    "    config = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"seq_len\": seq_len,\n",
    "        \"d_model\": d_model,\n",
    "        \"data_folder\": data_folder,\n",
    "        \"model_folder\": model_folder,\n",
    "        \"model_basename\": model_basename,\n",
    "        \"tokenizer_file\": tokenizer_file,\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"num_examples\": num_examples,\n",
    "        \"verbose\": verbose,\n",
    "        \"finetune\": finetune\n",
    "    }\n",
    "    train(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3f36ae-c7c4-49d0-8166-5250e27102a9",
   "metadata": {},
   "source": [
    "# Define Pipeline\n",
    "----\n",
    "\n",
    "A pipeline that downloads data and then trains a model based off the top performer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16788ec7-2304-4fcc-9725-e8ad54c80e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@pipeline(name='download-reddit')\n",
    "def download_preprocess_train_pipeline(\n",
    "    batch_size: int=8,\n",
    "    num_epochs: int=400,\n",
    "    lr: float=0.0001, \n",
    "    seq_len: int=560,\n",
    "    d_model: int=512, \n",
    "    data_folder: str=\"/data\",\n",
    "    model_folder: str=\".\", \n",
    "    model_basename: str=\"tmodel\", \n",
    "    tokenizer_file: str=\"\", \n",
    "    experiment_name: str=\"reddit\", \n",
    "    finetune: str=\"\",\n",
    "    num_examples: int=5, \n",
    "    verbose: bool=False,\n",
    "    test_window: int=2\n",
    "):\n",
    "    import os\n",
    "    from kfp import kubernetes\n",
    "    # DOWNLOAD\n",
    "    pvc1 = kubernetes.CreatePVC(\n",
    "        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "        pvc_name='reddit-pvc',\n",
    "        access_modes=['ReadWriteOnce'],\n",
    "        size='5Gi',\n",
    "        storage_class_name='microk8s-hostpath',\n",
    "    )\n",
    "    \n",
    "    download_task = download_dataset(\n",
    "    ).set_env_variable(name='REDDIT_CLIENT_ID', value=client_id) \\\n",
    "    .set_env_variable(name='REDDIT_CLIENT_PW', value=client_secret) \\\n",
    "    .set_env_variable(name='REDDIT_USER', value=username) \\\n",
    "    .set_env_variable(name='REDDIT_PW', value=password) \\\n",
    "    .set_env_variable(name='REDDIT_DB_URI', value=storage_uri)\n",
    "    download_task.set_caching_options(enable_caching=False)\n",
    "    kubernetes.mount_pvc(\n",
    "        download_task,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    test_task = test_model(\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr, \n",
    "        seq_len=seq_len,\n",
    "        d_model=d_model, \n",
    "        data_folder=data_folder,\n",
    "        model_folder=model_folder, \n",
    "        model_basename=model_basename, \n",
    "        tokenizer_file=\"\",\n",
    "        experiment_name=experiment_name, \n",
    "        num_examples=num_examples, \n",
    "        verbose=verbose,\n",
    "        test_window=test_window\n",
    "    ).set_env_variable(name='HF_TOKEN2', value=os.getenv('HF_TOKEN2')) \\\n",
    "    .set_env_variable(name='MLFLOW_TRACKING_URI', value=mlflow_tracking_uri)\\\n",
    "    .set_env_variable(name='MLFLOW_S3_ENDPOINT_URL', value=mlflow_s3_endpoint_url)\\\n",
    "    .set_env_variable(name='AWS_ACCESS_KEY_ID', value=aws_access_key_id)\\\n",
    "    .set_env_variable(name='AWS_SECRET_ACCESS_KEY', value=aws_secret_access_key)\\\n",
    "    .set_gpu_limit(1) \\\n",
    "    .after(download_task)\n",
    "    test_task.set_caching_options(enable_caching=False)\n",
    "    kubernetes.mount_pvc(\n",
    "        test_task,\n",
    "        pvc_name=pvc1.outputs['name'],\n",
    "        mount_path='/data',\n",
    "    )\n",
    "    with dsl.If(test_task.output == True):\n",
    "        pvc2 = kubernetes.CreatePVC(\n",
    "            # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n",
    "            pvc_name='reddit-pvc',\n",
    "            access_modes=['ReadWriteOnce'],\n",
    "            size='5Gi',\n",
    "            storage_class_name='microk8s-hostpath',\n",
    "        )\n",
    "        training_task = train_reddit_model(\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr, \n",
    "            seq_len=seq_len,\n",
    "            d_model=d_model, \n",
    "            data_folder=data_folder,\n",
    "            model_folder=model_folder, \n",
    "            model_basename=model_basename, \n",
    "            tokenizer_file=\"\",\n",
    "            experiment_name=experiment_name, \n",
    "            num_examples=num_examples, \n",
    "            verbose=verbose,\n",
    "            finetune=finetune\n",
    "        ).set_env_variable(name='HF_TOKEN2', value=os.getenv('HF_TOKEN2')) \\\n",
    "        .set_env_variable(name='MLFLOW_TRACKING_URI', value=mlflow_tracking_uri)\\\n",
    "        .set_env_variable(name='MLFLOW_S3_ENDPOINT_URL', value=mlflow_s3_endpoint_url)\\\n",
    "        .set_env_variable(name='AWS_ACCESS_KEY_ID', value=aws_access_key_id)\\\n",
    "        .set_env_variable(name='AWS_SECRET_ACCESS_KEY', value=aws_secret_access_key)\\\n",
    "        .set_gpu_limit(1)\n",
    "        training_task.set_caching_options(enable_caching=False)\n",
    "        kubernetes.mount_pvc(\n",
    "            training_task,\n",
    "            pvc_name=pvc2.outputs['name'],\n",
    "            mount_path='/data',\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd1942-95cc-4e19-ac7f-ff71e9a5cbc3",
   "metadata": {},
   "source": [
    "## Manually run pipeline\n",
    "\n",
    "---\n",
    "\n",
    "Useful for testing the downloading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9382aef3-745a-445e-8960-0a567b2b6cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/4e89aa90-7cd8-4601-b798-0cf56e70e584\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/5fc92128-dfbf-4c33-a540-62a08685f1f0\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.create_run_from_pipeline_func(download_preprocess_train_pipeline, arguments={\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 500,\n",
    "    \"lr\": 1e-4,\n",
    "    \"seq_len\": 560,\n",
    "    \"d_model\": 512,\n",
    "    \"model_folder\": \"weights\",\n",
    "    \"data_folder\": \"/data\",\n",
    "    \"model_basename\": \"tmodel_\",\n",
    "    # \"optimizer_folder\": \"optimizer\",\n",
    "    \"experiment_name\": \"tmodel\",\n",
    "    \"tokenizer_file\": \"\",\n",
    "    \"num_examples\": 5,\n",
    "    \"finetune\": \"\",\n",
    "    \"verbose\": False,\n",
    "    \"test_window\": 2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f1c11-3bd2-4278-ab0d-6503a8ae21c8",
   "metadata": {},
   "source": [
    "## Register pipeline\n",
    "\n",
    "---\n",
    "\n",
    "Used to schedule the task to run daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d23e90a-8de1-4a0f-a67f-5ddd82d3166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/pipelines/details/aaed8189-ac5d-432b-8ff9-dad92581af59\" target=\"_blank\" >Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kfp.compiler.Compiler().compile(download_preprocess_train_pipeline, package_path='pipeline.yaml')\n",
    "pipeline_info = client.upload_pipeline(\"pipeline.yaml\", 'download-train-reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc3a8d24-f5c1-4d5c-a6ac-d07f70ba2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_version = [ver for ver in client.list_pipeline_versions(pipeline_info.pipeline_id).pipeline_versions if ver.display_name=='download-train-reddit'][0]\n",
    "pipeline_version_id = pipeline_version.pipeline_version_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc528f72-dd5e-4151-aee8-5d1f8e19d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = [exp for exp in client.list_experiments().experiments if exp.display_name==\"default\"][0]\n",
    "experiment_id = experiment.experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a8a71f11-096d-48a8-9d90-aa5a2a77fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recurring_run = client.create_recurring_run(\n",
    "    experiment_id=experiment_id,\n",
    "    pipeline_id=pipeline_info.pipeline_id,\n",
    "    version_id=pipeline_version_id,\n",
    "    cron_expression=\"0 0 * * *\",\n",
    "    job_name=\"Daily Training\",\n",
    "    params={\n",
    "        \"num_epochs\":100,\n",
    "        \"batch_size\":4,\n",
    "        \"learning_rate\":0.001,\n",
    "        \"finetune\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962cb82-2031-485f-b27e-9edc55227cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

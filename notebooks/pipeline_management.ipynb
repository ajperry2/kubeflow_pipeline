{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8e5895-30a8-4a2b-82d2-d77ada5542f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install praw mlflow kserve tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1b3128-ac06-4190-aba3-63fa64c03131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import os\n",
    "from kfp.v2.dsl import importer, Metrics\n",
    "from kfp.dsl import Input, component\n",
    "from kfp.dsl import OutputPath, pipeline\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset, Output\n",
    "\n",
    "\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911fc81-333c-4530-bebd-7fa43bfb55df",
   "metadata": {},
   "source": [
    "# Downloading Data\n",
    "----\n",
    "\n",
    "This component downloads data from reddit and saves them to the datastore for down stream tasks. \n",
    "\n",
    "It is ran daily to collect the daily limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c7e24-3b0b-4baf-b9bb-64b9c3783e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"praw\"]\n",
    ")\n",
    "def download_dataset(dataset_path: OutputPath('Dataset')) -> None:\n",
    "    import praw\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import os\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_PW'),\n",
    "        user_agent=\"YOUR_USER_AGENT\",\n",
    "        # Optional: For authenticated requests\n",
    "        username=os.getenv('REDDIT_USER'),\n",
    "        password=os.getenv('REDDIT_PW')\n",
    "    )\n",
    "    subreddit_names = [\n",
    "        \"funny\",\n",
    "        \"AskReddit\",\n",
    "        \"gaming\",\n",
    "        \"worldnews\",\n",
    "        \"todayilearned\",\n",
    "        \"Music\",\n",
    "        \"aww\",\n",
    "        \"movies\",\n",
    "        \"memes\",\n",
    "        \"science\"\n",
    "    ]\n",
    "    data_path = Path(dataset_path)\n",
    "    data_path.mkdir(exist_ok=True)\n",
    "\n",
    "    for subreddit_name in subreddit_names:\n",
    "        print(subreddit_name)\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        for submission in subreddit.top(time_filter=\"day\", limit=10):\n",
    "        \n",
    "            most_upvoted_comment = None\n",
    "            highest_score = -1\n",
    "            \n",
    "            for comment in submission.comments.list():\n",
    "                if not hasattr(comment, \"author\") or not comment.author:  # Skip deleted comments\n",
    "                    continue\n",
    "                if comment.score > highest_score:\n",
    "                    highest_score = comment.score\n",
    "                    most_upvoted_comment = comment\n",
    "            text = submission.selftext_html\n",
    "            if most_upvoted_comment is not None:\n",
    "                comment_text = most_upvoted_comment.body.strip()\n",
    "                comment_score = most_upvoted_comment.score\n",
    "            else:\n",
    "                comment_text = \"\"\n",
    "                comment_score = 0\n",
    "            url = submission.url\n",
    "            title = submission.title\n",
    "            id  = submission.id\n",
    "            print(\"\\tid\")\n",
    "            submission_data = {\n",
    "                \"id\": id,\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"text\": text,\n",
    "                \"top_comment\": comment_text,\n",
    "                \"comment_score\": comment_score\n",
    "            }\n",
    "            with (data_path / f\"{id}.json\").open(\"w+\") as f:\n",
    "                json.dump(submission_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16788ec7-2304-4fcc-9725-e8ad54c80e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISVC_NAME = \"reddit-model\"\n",
    "MLFLOW_RUN_NAME = \"reddit_models\"\n",
    "MLFLOW_MODEL_NAME = \"reddit-model\"\n",
    "\n",
    "# mlflow_tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "# mlflow_s3_endpoint_url = os.getenv('MLFLOW_S3_ENDPOINT_URL')\n",
    "# aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "# aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "client_id=os.getenv('REDDIT_CLIENT_ID')\n",
    "client_secret=os.getenv('REDDIT_CLIENT_PW')\n",
    "# Optional: For authenticated requests\n",
    "username=os.getenv('REDDIT_USER')\n",
    "password=os.getenv('REDDIT_PW')\n",
    "\n",
    "@pipeline(name='download-reddit')\n",
    "def download_preprocess_train_deploy_pipeline():\n",
    "    download_task = download_dataset(\n",
    "    ).set_env_variable(name='REDDIT_CLIENT_ID', value=client_id) \\\n",
    "    .set_env_variable(name='REDDIT_CLIENT_PW', value=client_secret) \\\n",
    "    .set_env_variable(name='REDDIT_USER', value=username) \\\n",
    "    .set_env_variable(name='REDDIT_PW', value=password)\n",
    "    print(download_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd1942-95cc-4e19-ac7f-ff71e9a5cbc3",
   "metadata": {},
   "source": [
    "## Manually run download task\n",
    "\n",
    "---\n",
    "\n",
    "Useful for testing the downloading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9382aef3-745a-445e-8960-0a567b2b6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = client.create_run_from_pipeline_func(download_preprocess_train_deploy_pipeline, arguments={}, enable_caching=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f1c11-3bd2-4278-ab0d-6503a8ae21c8",
   "metadata": {},
   "source": [
    "## Register pipeline\n",
    "\n",
    "---\n",
    "\n",
    "Used to schedule the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d23e90a-8de1-4a0f-a67f-5ddd82d3166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfp.compiler.Compiler().compile(download_preprocess_train_deploy_pipeline, package_path='pipeline.yaml')\n",
    "# client.upload_pipeline(\"pipeline.yaml\", 'download-reddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fb676-7bcd-42fb-95b7-9e575e735504",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "----\n",
    "\n",
    "Train a model using GPU. A customizable number of epochs, learning rate, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba793efb-e7b9-49d2-844c-c05f36912467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(\n",
    "    base_image=\"wallies/python-cuda:3.10-cuda11.6-runtime\",\n",
    "    packages_to_install=[\"ajperry_pipeline\"]\n",
    ")\n",
    "def train_model(\n",
    "    input_dataset: Input[Dataset], \n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    from ajperry_pipeline.ml.models import Transformer\n",
    "    from ajperry_pipeline.ml.data.reddit import RedditDataset\n",
    "    \n",
    "\n",
    "    device=\"cuda\"\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    training_dataset = RedditDataset(\n",
    "        input_dataset.path,\n",
    "        is_train=True, \n",
    "        train_split_perc=0.8\n",
    "    )\n",
    "    test_dataset = RedditDataset(\n",
    "        input_dataset.path,\n",
    "        is_train=False, \n",
    "        train_split_perc=0.8\n",
    "    )\n",
    "    print(f\"Training Samples: {len(training_dataset)}\")\n",
    "    print(f\"Testing Samples: {len(test_dataset)}\")\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        training_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    transformer = Transformer(\n",
    "        embedding_model=\"bert-base-uncased\", \n",
    "        num_heads= 2,\n",
    "        num_encoders= 6,\n",
    "        num_nn_layers = 3,\n",
    "        embedding_size = 128,\n",
    "        device = device,\n",
    "        max_length = 20\n",
    "    )\n",
    "    optimizer = optim.SGD(transformer.parameters(), lr=learning_rate)\n",
    "    \n",
    "    transformer = transformer.to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    optimizer = optim.SGD(transformer.parameters(), lr=0.01)\n",
    "    \n",
    "    # Training loop (one optimization step shown)\n",
    "    for epoch in range(num_epochs): # Illustrative single epoch\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        transformer.train()\n",
    "        for titles, top_comments in train_dataloader:\n",
    "            # 1. Forward pass\n",
    "            output_logits, outputs, ended = transformer(titles)\n",
    "            actuals = torch.tensor([transformer.tokenizer.encode(top_comments[i],truncation=True, padding='max_length', max_length=transformer.max_length) for i in range(len(top_comments))])\n",
    "            actuals_onehot = torch.nn.functional.one_hot(actuals, num_classes=transformer.tokenizer.vocab_size)\n",
    "            logits = torch.stack([torch.stack(logit) for logit in output_logits])\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, actuals_onehot.float().to(device))\n",
    "            for i, e in enumerate(ended):\n",
    "                loss[i,e:] = 0.0\n",
    "            loss = loss.mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # # 4. Update parameters\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        transformer.eval()\n",
    "        with torch.no_grad():\n",
    "            for titles, top_comments in test_dataloader:\n",
    "                # 1. Forward pass\n",
    "                output_logits, outputs, ended = transformer(titles)\n",
    "                actuals = torch.tensor([transformer.tokenizer.encode(top_comments[i],truncation=True, padding='max_length', max_length=transformer.max_length) for i in range(len(top_comments))])\n",
    "                actuals_onehot = torch.nn.functional.one_hot(actuals, num_classes=transformer.tokenizer.vocab_size)\n",
    "                logits = torch.stack([torch.stack(logit) for logit in output_logits])\n",
    "                # Calculate the loss\n",
    "                loss = criterion(logits, actuals_onehot.float().to(device))\n",
    "                for i, e in enumerate(ended):\n",
    "                    loss[i,e:] = 0.0\n",
    "                loss = loss.mean()\n",
    "                test_losses.append(loss.item())\n",
    "        average_train_loss = sum(train_losses) / len(train_losses)\n",
    "        average_test_loss = sum(test_losses) / len(test_losses)\n",
    "        metrics.log_metric('train_bce', average_loss)\n",
    "        metrics.log_metric('test_bce', average_test_loss)\n",
    "\n",
    "@dsl.pipeline(name='model-train-pipeline')\n",
    "def train_model_pipeline(\n",
    "    dataset_uri: str,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "):\n",
    "    # Import the dataset artifact from the specified URI\n",
    "    import os\n",
    "    # Import the dataset artifact from the specified URI\n",
    "    imported_dataset_task = importer(\n",
    "        artifact_uri=dataset_uri,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False # Set to True if you want a new ML Metadata entry\n",
    "    )\n",
    "    train_model(\n",
    "        input_dataset=imported_dataset_task.outputs['artifact'],\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "    ).set_env_variable(name='HF_TOKEN2', value=os.getenv('HF_TOKEN2')) \\\n",
    "    .set_gpu_limit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148bac7-e960-4b23-93af-df271fdc7861",
   "metadata": {},
   "source": [
    "## Manually run training task\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad24d1-7e18-4ffb-8f47-1668e0a89a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    train_model_pipeline, \n",
    "    arguments={\n",
    "        \"dataset_uri\":\"minio://mlpipeline/v2/artifacts/download-reddit/c1d12a96-5baf-410d-9e95-83333f005439/download-dataset/0dcdadc5-eb72-49ee-8e65-e4e7547eae86/dataset_path\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"batch_size\": 1,\n",
    "        \"learning_rate\": 0.01,\n",
    "    }, \n",
    "    enable_caching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386de9b-eff1-4d0a-b9ca-cce82721367c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

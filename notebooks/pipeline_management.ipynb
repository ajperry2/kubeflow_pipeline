{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8e5895-30a8-4a2b-82d2-d77ada5542f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "feast 0.49.0 requires uvicorn[standard]==0.34.0, but you have uvicorn 0.30.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install praw mlflow kserve tenacity -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d1b3128-ac06-4190-aba3-63fa64c03131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import os\n",
    "\n",
    "\n",
    "from kfp.dsl import Input, component\n",
    "from kfp.dsl import OutputPath, pipeline\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7c7e24-3b0b-4baf-b9bb-64b9c3783e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"praw\"]\n",
    ")\n",
    "def download_dataset(dataset_path: OutputPath('Dataset')) -> None:\n",
    "    import praw\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import os\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_PW'),\n",
    "        user_agent=\"YOUR_USER_AGENT\",\n",
    "        # Optional: For authenticated requests\n",
    "        username=os.getenv('REDDIT_USER'),\n",
    "        password=os.getenv('REDDIT_PW')\n",
    "    )\n",
    "    subreddit_names = [\n",
    "        \"funny\",\n",
    "        \"AskReddit\",\n",
    "        \"gaming\",\n",
    "        \"worldnews\",\n",
    "        \"todayilearned\",\n",
    "        \"Music\",\n",
    "        \"aww\",\n",
    "        \"movies\",\n",
    "        \"memes\",\n",
    "        \"science\"\n",
    "    ]\n",
    "    data_path = Path(dataset_path)\n",
    "    data_path.mkdir(exist_ok=True)\n",
    "\n",
    "    for subreddit_name in subreddit_names:\n",
    "        print(subreddit_name)\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        for submission in subreddit.top(time_filter=\"day\", limit=10):\n",
    "        \n",
    "            most_upvoted_comment = None\n",
    "            highest_score = -1\n",
    "            \n",
    "            for comment in submission.comments.list():\n",
    "                if not hasattr(comment, \"author\") or not comment.author:  # Skip deleted comments\n",
    "                    continue\n",
    "                if comment.score > highest_score:\n",
    "                    highest_score = comment.score\n",
    "                    most_upvoted_comment = comment\n",
    "            text = submission.selftext_html\n",
    "            if most_upvoted_comment is not None:\n",
    "                comment_text = most_upvoted_comment.body.strip()\n",
    "                comment_score = most_upvoted_comment.score\n",
    "            else:\n",
    "                comment_text = \"\"\n",
    "                comment_score = 0\n",
    "            url = submission.url\n",
    "            title = submission.title\n",
    "            id  = submission.id\n",
    "            print(\"\\tid\")\n",
    "            submission_data = {\n",
    "                \"id\": id,\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"text\": text,\n",
    "                \"top_comment\": comment_text,\n",
    "                \"comment_score\": comment_score\n",
    "            }\n",
    "            with (data_path / f\"{id}.json\").open(\"w+\") as f:\n",
    "                json.dump(submission_data, f)\n",
    "\n",
    "# @component(\n",
    "#     base_image=\"python:3.11\",\n",
    "#     packages_to_install=[\"pandas==2.2.2\", \"pyarrow==15.0.2\"]\n",
    "# )\n",
    "# def preprocess_dataset(dataset: InputPath('Dataset'), output_file: OutputPath('Dataset')) -> None:\n",
    "#     import pandas as pd\n",
    "\n",
    "#     df = pd.read_csv(dataset, header=0)\n",
    "#     df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "#     df.to_parquet(output_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16788ec7-2304-4fcc-9725-e8ad54c80e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISVC_NAME = \"reddit-model\"\n",
    "MLFLOW_RUN_NAME = \"reddit_models\"\n",
    "MLFLOW_MODEL_NAME = \"reddit-model\"\n",
    "\n",
    "# mlflow_tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "# mlflow_s3_endpoint_url = os.getenv('MLFLOW_S3_ENDPOINT_URL')\n",
    "# aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "# aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "client_id=os.getenv('REDDIT_CLIENT_ID')\n",
    "client_secret=os.getenv('REDDIT_CLIENT_PW')\n",
    "# Optional: For authenticated requests\n",
    "username=os.getenv('REDDIT_USER')\n",
    "password=os.getenv('REDDIT_PW')\n",
    "\n",
    "@pipeline(name='download-reddit')\n",
    "def download_preprocess_train_deploy_pipeline():\n",
    "    download_task = download_dataset(\n",
    "    ).set_env_variable(name='REDDIT_CLIENT_ID', value=client_id) \\\n",
    "    .set_env_variable(name='REDDIT_CLIENT_PW', value=client_secret) \\\n",
    "    .set_env_variable(name='REDDIT_USER', value=username) \\\n",
    "    .set_env_variable(name='REDDIT_PW', value=password)\n",
    "    print(download_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9382aef3-745a-445e-8960-0a567b2b6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = client.create_run_from_pipeline_func(download_preprocess_train_deploy_pipeline, arguments={}, enable_caching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d23e90a-8de1-4a0f-a67f-5ddd82d3166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfp.compiler.Compiler().compile(download_preprocess_train_deploy_pipeline, package_path='pipeline.yaml')\n",
    "# client.upload_pipeline(\"pipeline.yaml\", 'download-reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "955694e6-dd54-40a6-b626-a7ad347ff50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/pipelines/details/f98b6779-0e3c-4ca3-a356-68f52f2315ef\" target=\"_blank\" >Pipeline details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'created_at': datetime.datetime(2025, 8, 20, 4, 59, 45, tzinfo=tzlocal()),\n",
       " 'description': None,\n",
       " 'display_name': 'download-reddit',\n",
       " 'error': None,\n",
       " 'namespace': None,\n",
       " 'pipeline_id': 'f98b6779-0e3c-4ca3-a356-68f52f2315ef'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba793efb-e7b9-49d2-844c-c05f36912467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset\n",
    "\n",
    "from kfp.v2.dsl import importer\n",
    "\n",
    "@dsl.component\n",
    "def consume_dataset(input_dataset: Input[Dataset]):\n",
    "    from pathlib import Path\n",
    "    files = list(Path(input_dataset.path).glob(\"*\"))\n",
    "    print(\"Num Files:\", len(files))\n",
    "    # Show file names\n",
    "    for file in files:\n",
    "        print(file)\n",
    "    \n",
    "\n",
    "@dsl.pipeline(name='data-consumer-pipeline')\n",
    "def data_consumer_pipeline(dataset_uri: str):\n",
    "    # Import the dataset artifact from the specified URI\n",
    "    imported_dataset_task = importer(\n",
    "        artifact_uri=dataset_uri,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False # Set to True if you want a new ML Metadata entry\n",
    "    )\n",
    "    consume_dataset(input_dataset=imported_dataset_task.outputs['artifact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ad24d1-7e18-4ffb-8f47-1668e0a89a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/54fc2528-208e-45c2-ab64-39cf4b68d2c2\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/206a3558-6f3d-4fda-97e1-5965ac11a1e7\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    data_consumer_pipeline, \n",
    "    arguments={\"dataset_uri\":\"minio://mlpipeline/v2/artifacts/download-reddit/c1d12a96-5baf-410d-9e95-83333f005439/download-dataset/0dcdadc5-eb72-49ee-8e65-e4e7547eae86/dataset_path\"}, \n",
    "    enable_caching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3a065-72b9-4524-86b0-b08b2e494438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

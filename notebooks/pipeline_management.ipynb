{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8e5895-30a8-4a2b-82d2-d77ada5542f7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /opt/conda/lib/python3.11/site-packages (7.8.1)\n",
      "Requirement already satisfied: mlflow in /opt/conda/lib/python3.11/site-packages (2.15.1)\n",
      "Requirement already satisfied: kserve in /opt/conda/lib/python3.11/site-packages (0.15.2)\n",
      "Requirement already satisfied: tenacity in /opt/conda/lib/python3.11/site-packages (8.5.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in /opt/conda/lib/python3.11/site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /opt/conda/lib/python3.11/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.11/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.6.15)\n",
      "Requirement already satisfied: mlflow-skinny==2.15.1 in /opt/conda/lib/python3.11/site-packages (from mlflow) (2.15.1)\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.11/site-packages (from mlflow) (3.1.1)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.16.2)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in /opt/conda/lib/python3.11/site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.11/site-packages (from mlflow) (3.8)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.11/site-packages (from mlflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in /opt/conda/lib/python3.11/site-packages (from mlflow) (2.2.3)\n",
      "Requirement already satisfied: pyarrow<16,>=4.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow) (15.0.2)\n",
      "Requirement already satisfied: querystring-parser<2 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.11/site-packages (from mlflow) (1.15.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow) (2.0.38)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.11/site-packages (from mlflow) (3.1.6)\n",
      "Requirement already satisfied: gunicorn<23 in /opt/conda/lib/python3.11/site-packages (from mlflow) (22.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (8.2.1)\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (3.1.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (0.57.0)\n",
      "Requirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (7.2.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (1.34.1)\n",
      "Requirement already satisfied: packaging<25 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (24.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (4.25.8)\n",
      "Requirement already satisfied: pytz<2025 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (2024.2)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (6.0.2)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow-skinny==2.15.1->mlflow) (0.5.3)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /opt/conda/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow) (4.14.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in /opt/conda/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (2.40.3)\n",
      "Requirement already satisfied: blinker>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: itsdangerous>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow) (3.0.2)\n",
      "Requirement already satisfied: werkzeug>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.15.1->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (4.9.1)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny==2.15.1->mlflow) (3.23.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow) (3.2.3)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.15.1->mlflow) (0.55b1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<3->mlflow) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.11/site-packages (from rsa<5,>=3.1.4->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.15.1->mlflow) (0.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
      "Requirement already satisfied: cloudevents<2.0.0,>=1.6.2 in /opt/conda/lib/python3.11/site-packages (from kserve) (1.12.0)\n",
      "Requirement already satisfied: fastapi>=0.115.3 in /opt/conda/lib/python3.11/site-packages (from kserve) (0.115.12)\n",
      "Requirement already satisfied: grpc-interceptor<1.0.0,>=0.15.4 in /opt/conda/lib/python3.11/site-packages (from kserve) (0.15.4)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.60.0 in /opt/conda/lib/python3.11/site-packages (from kserve) (1.74.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.2 in /opt/conda/lib/python3.11/site-packages (from kserve) (0.27.2)\n",
      "Requirement already satisfied: kubernetes>=23.3.0 in /opt/conda/lib/python3.11/site-packages (from kserve) (30.1.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.15 in /opt/conda/lib/python3.11/site-packages (from kserve) (3.11.2)\n",
      "Requirement already satisfied: prometheus-client<0.21.0,>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from kserve) (0.20.0)\n",
      "Requirement already satisfied: psutil<6.0.0,>=5.9.6 in /opt/conda/lib/python3.11/site-packages (from kserve) (5.9.8)\n",
      "Requirement already satisfied: pydantic<3,>1.0 in /opt/conda/lib/python3.11/site-packages (from kserve) (2.10.6)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from kserve) (0.9.0)\n",
      "Requirement already satisfied: timing-asgi<0.4.0,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from kserve) (0.3.1)\n",
      "Requirement already satisfied: uvicorn<0.31.0,>=0.30.6 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]<0.31.0,>=0.30.6->kserve) (0.30.6)\n",
      "Requirement already satisfied: deprecation<3.0,>=2.0 in /opt/conda/lib/python3.11/site-packages (from cloudevents<2.0.0,>=1.6.2->kserve) (2.1.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.2->kserve) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.2->kserve) (1.0.9)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.2->kserve) (1.3.1)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.2->kserve) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>1.0->kserve) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>1.0->kserve) (2.27.2)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]<0.31.0,>=0.30.6->kserve) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]<0.31.0,>=0.30.6->kserve) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]<0.31.0,>=0.30.6->kserve) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]<0.31.0,>=0.30.6->kserve) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.11/site-packages (from uvicorn[standard]<0.31.0,>=0.30.6->kserve) (15.0.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/conda/lib/python3.11/site-packages (from fastapi>=0.115.3->kserve) (0.46.2)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes>=23.3.0->kserve) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=23.3.0->kserve) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw mlflow kserve tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d1b3128-ac06-4190-aba3-63fa64c03131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import os\n",
    "from kfp.v2.dsl import importer, Metrics\n",
    "from kfp.dsl import Input, component\n",
    "from kfp.dsl import OutputPath, pipeline\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Dataset, Output\n",
    "\n",
    "\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911fc81-333c-4530-bebd-7fa43bfb55df",
   "metadata": {},
   "source": [
    "# Downloading Data\n",
    "----\n",
    "\n",
    "This component downloads data from reddit and saves them to the datastore for down stream tasks. \n",
    "\n",
    "It is ran daily to collect the daily limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b7c7e24-3b0b-4baf-b9bb-64b9c3783e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"praw\"]\n",
    ")\n",
    "def download_dataset(dataset_path: OutputPath('Dataset')) -> None:\n",
    "    import praw\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import os\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
    "        client_secret=os.getenv('REDDIT_CLIENT_PW'),\n",
    "        user_agent=\"YOUR_USER_AGENT\",\n",
    "        # Optional: For authenticated requests\n",
    "        username=os.getenv('REDDIT_USER'),\n",
    "        password=os.getenv('REDDIT_PW')\n",
    "    )\n",
    "    subreddit_names = [\n",
    "        \"funny\",\n",
    "        \"AskReddit\",\n",
    "        \"gaming\",\n",
    "        \"worldnews\",\n",
    "        \"todayilearned\",\n",
    "        \"Music\",\n",
    "        \"aww\",\n",
    "        \"movies\",\n",
    "        \"memes\",\n",
    "        \"science\"\n",
    "    ]\n",
    "    data_path = Path(dataset_path)\n",
    "    data_path.mkdir(exist_ok=True)\n",
    "\n",
    "    for subreddit_name in subreddit_names:\n",
    "        print(subreddit_name)\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        for submission in subreddit.top(time_filter=\"day\", limit=10):\n",
    "        \n",
    "            most_upvoted_comment = None\n",
    "            highest_score = -1\n",
    "            \n",
    "            for comment in submission.comments.list():\n",
    "                if not hasattr(comment, \"author\") or not comment.author:  # Skip deleted comments\n",
    "                    continue\n",
    "                if comment.score > highest_score:\n",
    "                    highest_score = comment.score\n",
    "                    most_upvoted_comment = comment\n",
    "            text = submission.selftext_html\n",
    "            if most_upvoted_comment is not None:\n",
    "                comment_text = most_upvoted_comment.body.strip()\n",
    "                comment_score = most_upvoted_comment.score\n",
    "            else:\n",
    "                comment_text = \"\"\n",
    "                comment_score = 0\n",
    "            url = submission.url\n",
    "            title = submission.title\n",
    "            id  = submission.id\n",
    "            print(\"\\tid\")\n",
    "            submission_data = {\n",
    "                \"id\": id,\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"text\": text,\n",
    "                \"top_comment\": comment_text,\n",
    "                \"comment_score\": comment_score\n",
    "            }\n",
    "            with (data_path / f\"{id}.json\").open(\"w+\") as f:\n",
    "                json.dump(submission_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "16788ec7-2304-4fcc-9725-e8ad54c80e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ISVC_NAME = \"reddit-model\"\n",
    "MLFLOW_RUN_NAME = \"reddit_models\"\n",
    "MLFLOW_MODEL_NAME = \"reddit-model\"\n",
    "\n",
    "# mlflow_tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "# mlflow_s3_endpoint_url = os.getenv('MLFLOW_S3_ENDPOINT_URL')\n",
    "# aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "# aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "\n",
    "client_id=os.getenv('REDDIT_CLIENT_ID')\n",
    "client_secret=os.getenv('REDDIT_CLIENT_PW')\n",
    "# Optional: For authenticated requests\n",
    "username=os.getenv('REDDIT_USER')\n",
    "password=os.getenv('REDDIT_PW')\n",
    "\n",
    "@pipeline(name='download-reddit')\n",
    "def download_preprocess_train_deploy_pipeline():\n",
    "    download_task = download_dataset(\n",
    "    ).set_env_variable(name='REDDIT_CLIENT_ID', value=client_id) \\\n",
    "    .set_env_variable(name='REDDIT_CLIENT_PW', value=client_secret) \\\n",
    "    .set_env_variable(name='REDDIT_USER', value=username) \\\n",
    "    .set_env_variable(name='REDDIT_PW', value=password)\n",
    "    print(download_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd1942-95cc-4e19-ac7f-ff71e9a5cbc3",
   "metadata": {},
   "source": [
    "## Manually run download task\n",
    "\n",
    "---\n",
    "\n",
    "Useful for testing the downloading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9382aef3-745a-445e-8960-0a567b2b6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = client.create_run_from_pipeline_func(download_preprocess_train_deploy_pipeline, arguments={}, enable_caching=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3f1c11-3bd2-4278-ab0d-6503a8ae21c8",
   "metadata": {},
   "source": [
    "## Register pipeline\n",
    "\n",
    "---\n",
    "\n",
    "Used to schedule the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d23e90a-8de1-4a0f-a67f-5ddd82d3166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfp.compiler.Compiler().compile(download_preprocess_train_deploy_pipeline, package_path='pipeline.yaml')\n",
    "# client.upload_pipeline(\"pipeline.yaml\", 'download-reddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2fb676-7bcd-42fb-95b7-9e575e735504",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "----\n",
    "\n",
    "Train a model using GPU. A customizable number of epochs, learning rate, and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba793efb-e7b9-49d2-844c-c05f36912467",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@component(\n",
    "    base_image=\"wallies/python-cuda:3.10-cuda11.6-runtime\",\n",
    "    packages_to_install=[\"ajperry_pipeline\"]\n",
    ")\n",
    "def train_model(\n",
    "    input_dataset: Input[Dataset], \n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    from ajperry_pipeline.ml.models import Transformer\n",
    "    from ajperry_pipeline.ml.data.reddit import RedditDataset\n",
    "    \n",
    "\n",
    "    device=\"cuda\"\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    \n",
    "    training_dataset = RedditDataset(\n",
    "        input_dataset.path,\n",
    "        is_train=True, \n",
    "        train_split_perc=0.8\n",
    "    )\n",
    "    test_dataset = RedditDataset(\n",
    "        input_dataset.path,\n",
    "        is_train=False, \n",
    "        train_split_perc=0.8\n",
    "    )\n",
    "    print(f\"Training Samples: {len(training_dataset)}\")\n",
    "    print(f\"Testing Samples: {len(test_dataset)}\")\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        training_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    transformer = Transformer(\n",
    "        embedding_model=\"bert-base-uncased\", \n",
    "        num_heads= 2,\n",
    "        num_encoders= 6,\n",
    "        num_nn_layers = 3,\n",
    "        embedding_size = 128,\n",
    "        device = device,\n",
    "        max_length = 20\n",
    "    )\n",
    "    optimizer = optim.SGD(transformer.parameters(), lr=learning_rate)\n",
    "    \n",
    "    transformer = transformer.to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "    optimizer = optim.SGD(transformer.parameters(), lr=0.01)\n",
    "    \n",
    "    # Training loop (one optimization step shown)\n",
    "    for epoch in range(num_epochs): # Illustrative single epoch\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        transformer.train()\n",
    "        for titles, top_comments in train_dataloader:\n",
    "            # 1. Forward pass\n",
    "            output_logits, outputs, ended = transformer(titles)\n",
    "            actuals = torch.tensor([transformer.tokenizer.encode(top_comments[i],truncation=True, padding='max_length', max_length=transformer.max_length) for i in range(len(top_comments))])\n",
    "            actuals_onehot = torch.nn.functional.one_hot(actuals, num_classes=transformer.tokenizer.vocab_size)\n",
    "            logits = torch.stack([torch.stack(logit) for logit in output_logits])\n",
    "            # Calculate the loss\n",
    "            loss = criterion(logits, actuals_onehot.float().to(device))\n",
    "            for i, e in enumerate(ended):\n",
    "                loss[i,e:] = 0.0\n",
    "            loss = loss.mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # # 4. Update parameters\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        transformer.eval()\n",
    "        with torch.no_grad():\n",
    "            for titles, top_comments in test_dataloader:\n",
    "                # 1. Forward pass\n",
    "                output_logits, outputs, ended = transformer(titles)\n",
    "                actuals = torch.tensor([transformer.tokenizer.encode(top_comments[i],truncation=True, padding='max_length', max_length=transformer.max_length) for i in range(len(top_comments))])\n",
    "                actuals_onehot = torch.nn.functional.one_hot(actuals, num_classes=transformer.tokenizer.vocab_size)\n",
    "                logits = torch.stack([torch.stack(logit) for logit in output_logits])\n",
    "                # Calculate the loss\n",
    "                loss = criterion(logits, actuals_onehot.float().to(device))\n",
    "                for i, e in enumerate(ended):\n",
    "                    loss[i,e:] = 0.0\n",
    "                loss = loss.mean()\n",
    "                test_losses.append(loss.item())\n",
    "        average_train_loss = sum(train_losses) / len(train_losses)\n",
    "        average_test_loss = sum(test_losses) / len(test_losses)\n",
    "        metrics.log_metric('train_bce', average_loss)\n",
    "        metrics.log_metric('test_bce', average_test_loss)\n",
    "\n",
    "@dsl.pipeline(name='model-train-pipeline')\n",
    "def train_model_pipeline(\n",
    "    dataset_uri: str,\n",
    "    num_epochs: int,\n",
    "    batch_size: int,\n",
    "    learning_rate: float,\n",
    "):\n",
    "    # Import the dataset artifact from the specified URI\n",
    "    import os\n",
    "    # Import the dataset artifact from the specified URI\n",
    "    imported_dataset_task = importer(\n",
    "        artifact_uri=dataset_uri,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False # Set to True if you want a new ML Metadata entry\n",
    "    )\n",
    "    train_model(\n",
    "        input_dataset=imported_dataset_task.outputs['artifact'],\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "    ).set_env_variable(name='HF_TOKEN2', value=os.getenv('HF_TOKEN2')) \\\n",
    "    .set_gpu_limit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4148bac7-e960-4b23-93af-df271fdc7861",
   "metadata": {},
   "source": [
    "## Manually run training task\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42ad24d1-7e18-4ffb-8f47-1668e0a89a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/54fc2528-208e-45c2-ab64-39cf4b68d2c2\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/e9c8914b-3f56-4d9e-90b6-da0ea72adbe3\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = client.create_run_from_pipeline_func(\n",
    "    train_model_pipeline, \n",
    "    arguments={\n",
    "        \"dataset_uri\":\"minio://mlpipeline/v2/artifacts/download-reddit/c1d12a96-5baf-410d-9e95-83333f005439/download-dataset/0dcdadc5-eb72-49ee-8e65-e4e7547eae86/dataset_path\",\n",
    "        \"num_epochs\": 100,\n",
    "        \"batch_size\": 1,\n",
    "        \"learning_rate\": 0.01,\n",
    "    }, \n",
    "    enable_caching=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386de9b-eff1-4d0a-b9ca-cce82721367c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

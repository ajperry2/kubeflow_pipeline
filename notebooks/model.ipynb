{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb6876c-6c7d-44d5-8506-04d40b1f47fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.8.0 in /home/alan/miniconda3/lib/python3.13/site-packages (2.8.0)\n",
      "Requirement already satisfied: transformers==4.55.2 in /home/alan/miniconda3/lib/python3.13/site-packages (4.55.2)\n",
      "Requirement already satisfied: filelock in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/alan/miniconda3/lib/python3.13/site-packages (from torch==2.8.0) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/alan/miniconda3/lib/python3.13/site-packages (from transformers==4.55.2) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/alan/miniconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.2) (1.1.8)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/alan/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alan/miniconda3/lib/python3.13/site-packages (from jinja2->torch==2.8.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/alan/miniconda3/lib/python3.13/site-packages (from requests->transformers==4.55.2) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alan/miniconda3/lib/python3.13/site-packages (from requests->transformers==4.55.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alan/miniconda3/lib/python3.13/site-packages (from requests->transformers==4.55.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alan/miniconda3/lib/python3.13/site-packages (from requests->transformers==4.55.2) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==2.8.0 transformers==4.55.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a38452-0e01-4a26-b180-fb0abd34a8ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 164\u001b[39m\n\u001b[32m    159\u001b[39m         attentioned = \u001b[38;5;28mself\u001b[39m.layer_norm_1(attentioned + x)\n\u001b[32m    160\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer_norm_2(attentioned + \u001b[38;5;28mself\u001b[39m.feed_forward(attentioned))\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m transformer = \u001b[43mTransformer\u001b[49m(\n\u001b[32m    165\u001b[39m         embedding_model=\u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m    166\u001b[39m         num_heads= \u001b[32m2\u001b[39m,\n\u001b[32m    167\u001b[39m         num_encoders= \u001b[32m6\u001b[39m,\n\u001b[32m    168\u001b[39m         num_nn_layers = \u001b[32m3\u001b[39m,\n\u001b[32m    169\u001b[39m         embedding_size = \u001b[32m128\u001b[39m,\n\u001b[32m    170\u001b[39m         device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    171\u001b[39m         max_length = \u001b[32m20\u001b[39m\n\u001b[32m    172\u001b[39m     )\n\u001b[32m    175\u001b[39m t_input = [\u001b[33m\"\u001b[39m\u001b[33mhello how are \u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhello how are you im fine and you?\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    176\u001b[39m output_logits, outputs = transformer(t_input)\n",
      "\u001b[31mNameError\u001b[39m: name 'Transformer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from math import sqrt, inf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Code from https://www.tensorflow.org/tutorials/text/transformer\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[None,...]\n",
    "    \n",
    "  return torch.tensor(pos_encoding)\n",
    "\n",
    "\n",
    "\n",
    "class SelfAttention(torch.nn.Module):\n",
    "    def __init__(self, token_dim: int, embedding_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.w_queries = [\n",
    "            torch.nn.Parameter(torch.rand(1,token_dim, embedding_size))\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "        self.w_keys = [\n",
    "            torch.nn.Parameter(torch.rand(1,token_dim, embedding_size))\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "        self.w_values = [\n",
    "            torch.nn.Parameter(torch.rand(1,token_dim, embedding_size))\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "        self.w_agg = torch.nn.Parameter(torch.rand(embedding_size*self.num_heads, token_dim))\n",
    "        self.embedding_size = embedding_size\n",
    "        self.token_dim = token_dim\n",
    "            \n",
    "    def forward(self, x, attention_mask):\n",
    "        attention_heads = []\n",
    "        for i in range(self.num_heads):\n",
    "\n",
    "            # b, s, token_dim -> b, s, embedding_size\n",
    "            Q = x @ self.w_queries[i] \n",
    "            K = x @ self.w_keys[i] \n",
    "            V = x @ self.w_values[i]\n",
    "            attention_heads.append(\n",
    "                torch.softmax(\n",
    "                    ((Q @ K.transpose(2,1))) / sqrt(self.embedding_size),\n",
    "                    dim=1\n",
    "                ) @ V)\n",
    "            \n",
    "        \n",
    "        multiple_heads = torch.cat(attention_heads, dim=-1)\n",
    "        return multiple_heads @ self.w_agg\n",
    "\n",
    "class FeedForwardNeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self, num_layers: int, token_dim: int, dropout_p = 0.0):\n",
    "        super().__init__()\n",
    "        self.linears = [torch.nn.Linear(token_dim,token_dim) for i in range(num_layers)]\n",
    "        self.dropout_p = dropout_p\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(negative_slope=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            if self.dropout_p > 0:\n",
    "                x = self.dropout(x)\n",
    "        return x\n",
    "        \n",
    "class EncoderDecoderAttention(torch.nn.Module):\n",
    "    def __init__(self, token_dim: int, embedding_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.w_queries = [\n",
    "            torch.nn.Parameter(torch.rand(1,token_dim, embedding_size))\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "        self.w_keys = [\n",
    "            torch.nn.Parameter(torch.rand(1,token_dim, embedding_size))\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "        self.w_values = [\n",
    "            torch.nn.Parameter(torch.rand(1,token_dim, embedding_size))\n",
    "            for i in range(self.num_heads)\n",
    "        ]\n",
    "        self.w_agg = torch.nn.Parameter(torch.rand(embedding_size*self.num_heads, token_dim))\n",
    "        self.embedding_size = embedding_size\n",
    "        self.token_dim = token_dim\n",
    "            \n",
    "    def forward(self, x, encoder_context, attention_mask, current_index):\n",
    "        attention_heads = []\n",
    "        zero_i = (attention_mask==0).nonzero()\n",
    "        for i in range(self.num_heads):\n",
    "            mask = torch.zeros(x.shape[0], x.shape[1], x.shape[1], dtype=torch.float)\n",
    "            for j in range(x.shape[0]):\n",
    "\n",
    "                min_col = min(current_index+1, x.shape[1]-1)\n",
    "                mask[j, min_col:,:] = -inf\n",
    "            # b, s, token_dim -> b, s, embedding_size\n",
    "            Q = x @ self.w_queries[i] \n",
    "            K = encoder_context @ self.w_keys[i] \n",
    "            V = encoder_context @ self.w_values[i]\n",
    "            relations = Q @ K.transpose(2,1) + mask\n",
    "            print(mask)\n",
    "            attention_heads.append(\n",
    "                torch.softmax(\n",
    "                    relations / sqrt(self.embedding_size), \n",
    "                    dim=1\n",
    "                ) @ V)\n",
    "        multiple_heads = torch.cat(attention_heads, dim=-1)\n",
    "        return multiple_heads @ self.w_agg\n",
    "        \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, token_dim: int, embedding_size: int, num_heads: int, num_layers: int, dropout_p: float = 0.0):\n",
    "        super().__init__() \n",
    "        self.self_attention = SelfAttention(token_dim, embedding_size, num_heads)\n",
    "        self.encoder_decoder_attention = EncoderDecoderAttention(token_dim, embedding_size, num_heads)\n",
    "        self.feed_forward = FeedForwardNeuralNetwork(num_layers, token_dim, dropout_p=dropout_p)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm((token_dim,))\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm((token_dim,))\n",
    "        self.layer_norm_3 = torch.nn.LayerNorm((token_dim,))\n",
    "        \n",
    "    def forward(self, original_context, x, attention_mask , current_index):\n",
    "        attentioned = self.self_attention(x, attention_mask)\n",
    "        attentioned = self.layer_norm_1(attentioned + x)\n",
    "        attentioned_2 = self.encoder_decoder_attention(attentioned, original_context, attention_mask, current_index)\n",
    "        attentioned_2 = self.layer_norm_2(attentioned + attentioned_2)\n",
    "        return self.layer_norm_3(attentioned_2 + self.feed_forward(attentioned_2))\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, token_dim: int, embedding_size: int, num_heads: int, num_layers: int, dropout_p: float = 0.0):\n",
    "        super().__init__() \n",
    "        self.self_attention = SelfAttention(token_dim, embedding_size, num_heads)\n",
    "        self.feed_forward = FeedForwardNeuralNetwork(num_layers, token_dim, dropout_p=dropout_p)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm((token_dim,))\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm((token_dim,))\n",
    "        \n",
    "    def forward(self, x, attention_mask):\n",
    "        attentioned = self.self_attention(x, attention_mask)\n",
    "        attentioned = self.layer_norm_1(attentioned + x)\n",
    "        return self.layer_norm_2(attentioned + self.feed_forward(attentioned))\n",
    "        \n",
    "\n",
    "\n",
    "transformer = Transformer(\n",
    "        embedding_model=\"bert-base-uncased\", \n",
    "        num_heads= 2,\n",
    "        num_encoders= 6,\n",
    "        num_nn_layers = 3,\n",
    "        embedding_size = 128,\n",
    "        device=\"cpu\",\n",
    "        max_length = 20\n",
    "    )\n",
    "\n",
    "\n",
    "t_input = [\"hello how are \", \"hello how are you im fine and you?\"]\n",
    "output_logits, outputs, ended = transformer(t_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb33938-95f4-49f5-abf1-4f640366aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1a2ec-6255-45d0-8fbc-51e132d1c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8c15c-2ec9-4112-8867-62d7dc07ddd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output = \"\"\n",
    "current_token = -1\n",
    "outputs = [[None for _ in range(transformer.max_length)] for j in range(len(tokens))]\n",
    "output_logits = [[None for _ in range(transformer.max_length)] for j in range(len(tokens))]\n",
    "ended = [False for j in range(len(tokens))]\n",
    "i = 0\n",
    "while i < transformer.max_length:\n",
    "    print(\"b\")\n",
    "    output_tokens = transformer.tokenizer([output], return_tensors=\"pt\", padding='max_length', max_length=transformer.max_length)\n",
    "    outputs_tokens = transformer.embedding_model(**output_tokens)\n",
    "    token_embeddings = outputs_tokens.last_hidden_state\n",
    "    attention_mask = output_tokens['attention_mask']\n",
    "    \n",
    "    current_embedding = torch.clone(token_embeddings)\n",
    "    \n",
    "    for decoder in transformer.decoders:\n",
    "        current_embedding = decoder(token_embeddings, current_embedding, attention_mask, i)\n",
    "    # to vocab size\n",
    "    logits = current_embedding.view(current_embedding.shape[0], -1) @ transformer.linear \n",
    "    probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    # fill in outputs\n",
    "    for j in range(len(tokens)):\n",
    "        outputs[j][i] = transformer.tokenizer.decode(preds, skip_special_tokens=True)\n",
    "        if outputs[j][i] == transformer.tokenizer.eos_token:\n",
    "            # This sentence is ended\n",
    "            ended[j] = True\n",
    "        output_logits[j][i] = probs.squeeze()\n",
    "    if all(ended):\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e5a15-8a9f-409c-94fb-b79197ab8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a91df-2fef-431f-b7b4-f689860ddf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.tokenizer.decode(preds, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcab5a61-3d25-4356-aad0-e8dd7a3aac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.nn.functional.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabf4bf-42b0-4c24-a9fa-86374b5b302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = probs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c55404c-ad88-42d3-b3db-2da22a986bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_token = transformer.tokenizer.decode(preds, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3557ba16-ef38-495f-a62c-c7b74f4522eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b130b43-c544-487d-a145-6b03b8bfabc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "    probs = torch.nn.functional.softmax(logits)\n",
    "    preds = probs.argmax(1)\n",
    "    current_token = transformer.tokenizer.decode(preds)\n",
    "    output += current_token \n",
    "    print(\"a\",current_token)\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4febb-07e1-43db-89b9-02e7c7e6cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_input = [\"hello how are \", \"hello how are you im fine and you?\"]\n",
    "tokens = transformer.tokenizer(t_input, return_tensors=\"pt\", padding='max_length', max_length=transformer.max_length)\n",
    "outputs = transformer.embedding_model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e041a55-4cf0-4edd-a0a7-493d7bb22477",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]]).float()\n",
    "\n",
    "lower_triangular_mask = torch.ones_like(matrix, dtype=torch.bool).tril(diagonal=0)\n",
    "matrix[~lower_triangular_mask] = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc032054-9d9a-4fb3-8f22-08c7d5b60a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afae76-6634-4042-b313-decf30667aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_input = [\"hello how are \", \"hello how are you im fine and you?\"]\n",
    "tokens = transformer(t_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a891873-d004-4cc4-9336-29d0b3819d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_input = [\"hello how are \", \"hello how are you im fine and you?\"]\n",
    "tokens = transformer.tokenizer(t_input, return_tensors=\"pt\", padding='max_length', max_length=transformer.max_length)\n",
    "outputs = transformer.embedding_model(**tokens)\n",
    "token_embeddings = outputs.last_hidden_state\n",
    "positional_embeddings = positional_encoding(*token_embeddings.shape[-2:])\n",
    "token_embeddings += positional_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae2ebd-cdeb-4bc4-beb1-273350d4cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a5e347-a0ec-4db9-bb2e-5cc3a5e24e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = transformer.tokenizer(t_input, return_tensors=\"pt\", padding='max_length', max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0297ed7-e588-4b65-9106-8f4e4c3aa2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acfd4a1-ca93-4421-b9c3-6ef145eba87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = transformer.embedding_model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64b6b2-51af-4337-a553-9d06f4819049",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(transformer.token_dim, transformer.embedding_size, 3,3).to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b347d-1c40-4b4a-bf31-62750a757fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoder(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc56ac5-693e-41ae-805f-7595b5751370",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed613a1-b714-4d27-8b83-19b70a0973b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594cd1f-dd21-4b60-b2e9-3f8373f182f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e9be0-f679-49d4-9a59-66ebf3bbf214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "SCOPES = ['https://www.googleapis.com/auth/userinfo.email', 'openid']\n",
    "flow = InstalledAppFlow.from_client_secrets_file('client_secret.json', SCOPES)\n",
    "credentials = flow.run_local_server(port=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b3867-28ba-4f91-ab32-24c08581ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac8f28-e79a-4a85-b611-f883e5a82d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
